{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 1 GPU(s) available.\nWe will use the GPU: GeForce GTX 1080\n"
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of training questions: 10000\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        __id__                                          __title__  \\\n302   61925401  How to make typescript transfer .pug file to c...   \n7625  61979054                            Why does it show Jetty?   \n1938  61828582  Assign service principal Admin Role on Service...   \n164   61903553  Adding custom properties for each request in A...   \n2619  61920007      How to make Jest spOn second call of function   \n\n                                               __tags__  \n302      typescript|firebase|google-cloud-functions|pug  \n7625                            java|scala|jetty|http4s  \n1938                               azure-service-fabric  \n164   asp.net|vb.net|azure-application-insights|c#-t...  \n2619             javascript|node.js|unit-testing|jestjs  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>__id__</th>\n      <th>__title__</th>\n      <th>__tags__</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>302</th>\n      <td>61925401</td>\n      <td>How to make typescript transfer .pug file to c...</td>\n      <td>typescript|firebase|google-cloud-functions|pug</td>\n    </tr>\n    <tr>\n      <th>7625</th>\n      <td>61979054</td>\n      <td>Why does it show Jetty?</td>\n      <td>java|scala|jetty|http4s</td>\n    </tr>\n    <tr>\n      <th>1938</th>\n      <td>61828582</td>\n      <td>Assign service principal Admin Role on Service...</td>\n      <td>azure-service-fabric</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>61903553</td>\n      <td>Adding custom properties for each request in A...</td>\n      <td>asp.net|vb.net|azure-application-insights|c#-t...</td>\n    </tr>\n    <tr>\n      <th>2619</th>\n      <td>61920007</td>\n      <td>How to make Jest spOn second call of function</td>\n      <td>javascript|node.js|unit-testing|jestjs</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "with open(\"../../datasets/post-questions.json\", 'r') as f:\n",
    "    df = pd.read_json(json.load(f))\n",
    "\n",
    "questions = df[[\"id\", \"title\", \"tags\"]]\n",
    "questions = questions.rename(columns={\"id\":\"__id__\", \"title\":\"__title__\",\"tags\":\"__tags__\"})\n",
    "print(f\"Number of training questions: {len(questions)}\")\n",
    "questions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The number of unique tags is 5308\n"
    }
   ],
   "source": [
    "#now we set up our categorical variables that we will be predicting. This will end up being a huge dataframe with a column for every unique tag and a row for every value, however with this we will be able to accurately predict which categories every title belongs to\n",
    "tags = []\n",
    "for group in questions.__tags__.values:\n",
    "    tag_list = group.split('|')\n",
    "    for tag in tag_list:\n",
    "        if (tag not in tags):\n",
    "            tags.append(tag)\n",
    "print(f\"The number of unique tags is {len(tags)}\")\n",
    "\n",
    "for tag in tags:\n",
    "    questions[tag] = 0\n",
    "\n",
    "for index, value in enumerate(questions.__tags__.values):\n",
    "    tag_list = value.split('|')\n",
    "    for tag in tag_list:\n",
    "        questions[tag][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "linux  split  rename  nested-loops  api  symfony4  sonata-admin  django  \\\n0      1      1       1             1    0         0             0       0   \n\n   struct  javascript  ...  sub-array  xcode11.4  hibernate-mapping  \\\n0       0           0  ...          0          0                  0   \n\n   google-cloud-endpoints-v2  office365-apps  rhandsontable  condor  \\\n0                          0               0              0       0   \n\n   augmented-reality  agora.io  reflect  \n0                  0         0        0  \n\n[1 rows x 5308 columns]\n"
    }
   ],
   "source": [
    "titles = questions.__title__.values\n",
    "\n",
    "# labels is going to be a list of lists with each index corresponding to a classification\n",
    "labels = []\n",
    "classification = questions.drop([\"__id__\", \"__title__\", \"__tags__\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading BERT Tokenizer\n"
    }
   ],
   "source": [
    "print(\"Loading BERT Tokenizer\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original:  rename files which produced by split\nTokenized:  ['ren', '##ame', 'files', 'which', 'produced', 'by', 'split']\nToken IDs:  [14916, 14074, 6764, 2029, 2550, 2011, 3975]\n"
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', titles[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(titles[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(titles[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Max sentence length is 64 tokens\n"
    }
   ],
   "source": [
    "max_len = 64\n",
    "\n",
    "for title in titles:\n",
    "    input_ids = tokenizer.encode(title, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    \n",
    "max_len = min(max_len, 512) #BERT's maximum input length\n",
    "print(f\"Max sentence length is {max_len} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for title in titles:\n",
    "        # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        title,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # Add the encoded sentence to the list.  \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    # And its attention mask (simply differentiates padding from non-padding)\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert lists back into tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitrnlpptconda2b08e866b02442bcb7052849d949335e",
   "display_name": "Python 3.7.7 64-bit ('rnlppt': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}