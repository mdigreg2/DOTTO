{"forloop": "2\nfor( &1 in range(&2, &3)):\n\tpass\n", "joeMamma": "3\n            print(\"It seems as if Mike has succeded in adding this to the command dictionary\")\n            print(\"It also seems as if this is a snippet of python code in a java file\")\n    ", "joeMummy": "4\n        print(\"pridhvi eats cake\"):\n        for i in range(yourmom)\n            hello\n", "neuralNetwork": "52\n    import tensorflow.compat.v1 as tf\n    from tensorflow.examples.tutorials.mnist import input_data\n\n    tf.disable_v2_behavior()\n    mnist = input_data.read_data_sets(\"./datasets/\", one_hot=True)\n    \n    batch_size = 100\n    x = tf.placeholder('float', [None, 784])\n    y = tf.placeholder('float')\n\n    def neural_network_model(data, layer_sizes):\n\n        hidden_layers = []\n        for i in range(len(layer_sizes)-1):\n            hidden_layers.append({'weights':tf.Variable(tf.random_normal([layer_sizes[i], layer_sizes[i+1]])), \n                                'biases':tf.Variable(tf.random_normal([layer_sizes[i+1]]))})\n\n        layer_outputs = []\n        layer_outputs.append(tf.add(tf.matmul(data, hidden_layers[0]['weights']), hidden_layers[0]['biases']))\n        layer_outputs[0] = tf.nn.relu(layer_outputs[0])\n    \n        for i in range(1, len(layer_sizes) - 2):\n            layer_outputs.append(tf.add(tf.matmul(layer_outputs[i-1], hidden_layers[i]['weights']), hidden_layers[i]['biases']))\n            layer_outputs[i] = tf.nn.relu(layer_outputs[i])\n    \n        layer_outputs.append(tf.add(tf.matmul(layer_outputs[len(layer_sizes)-3], hidden_layers[len(layer_sizes)-2]['weights']), hidden_layers[len(layer_sizes)-2]['biases']))\n        \n        return layer_outputs[len(layer_sizes)-2]\n\n    def train_neural_network(x, layer_sizes):\n        prediction = neural_network_model(x, layer_sizes)\n        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n        n_epochs = 10\n\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        \n        for epoch in range(n_epochs):\n            epoch_loss = 0 #cost\n            #super high level tensorflow training magic\n            for _ in range(int(mnist.train.num_examples/batch_size)):\n                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n                _, c = sess.run([optimizer, cost], feed_dict={x:epoch_x, y:epoch_y})\n                epoch_loss += c\n            print('Epoch ', epoch, ' completed out of ', n_epochs, ' loss: ', epoch_loss)\n      \n        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1)) #gives the max of those matrixes\n        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))  #casts the corrext to a float\n        print(\"Accuracy: \", accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n\ntrain_neural_network(x, [784,500,500,500,10])\n", "load_from_yahoo": "5\ndef load_from_yahoo(tickers):\n    output = dict()\n    for ticker in tickers:\n        output[ticker] = yf.download(ticker, period=\"max\")\n    return output\n", "load_to_pickle_batch": "12\ndef load_to_pickle_batch(path, tickers, batch_size):\n\n    counter = 0\n    temp_path = path\n    for i in range(int(len(tickers)/batch_size)):\n        temp_path = path.split(\"/\")\n        temp_path[-1] = str(i) + \"_\" + temp_path[-1]\n        temp_path = \"/\".join(temp_path)\n        with open(temp_path, 'wb+') as file:\n            pickle.dump(tickers[counter:(counter+batch_size)], file)\n        counter += batch_size\n        print(\"Finished batch: \" + str(i+1) + \" of \" + str(int(len(tickers)/batch_size)))\n"}